{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75a06ae9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6c351f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (4.20.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (2022.7.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pandas in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: adapter-transformers in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (4.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (2022.7.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (0.8.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (0.0.53)\n",
      "Requirement already satisfied: filelock in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (3.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from adapter-transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from packaging>=20.0->adapter-transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from tqdm>=4.27->adapter-transformers) (0.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from importlib-metadata->adapter-transformers) (3.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->adapter-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->adapter-transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->adapter-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from requests->adapter-transformers) (2022.6.15)\n",
      "Requirement already satisfied: click in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from sacremoses->adapter-transformers) (8.1.3)\n",
      "Requirement already satisfied: six in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from sacremoses->adapter-transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from sacremoses->adapter-transformers) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sudeshna_dash\\miniconda3\\envs\\transformer-adapter\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install adapter-transformers\n",
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1252ea63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32331651",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r'data\\rct_sample_train.jsonl', lines=True)\n",
    "df = df.drop('metadata', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2968fad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Use of the mobile application was greater than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Between-group effect sizes were 0.78 ( P &lt; .00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Future investigations on the efficacy and safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>GLP-1 and BNP were infused in incremental dose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Women compared with men had higher ischemic st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "0  CONCLUSIONS  Use of the mobile application was greater than...\n",
       "1      RESULTS  Between-group effect sizes were 0.78 ( P < .00...\n",
       "2  CONCLUSIONS  Future investigations on the efficacy and safe...\n",
       "3      RESULTS  GLP-1 and BNP were infused in incremental dose...\n",
       "4      RESULTS  Women compared with men had higher ischemic st..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c453d2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONCLUSIONS' 'RESULTS' 'OBJECTIVE' 'METHODS' 'BACKGROUND']\n"
     ]
    }
   ],
   "source": [
    "labels_df = df.label.unique()\n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95980a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5834773ef589f384\n",
      "Reusing dataset json (C:\\Users\\Sudeshna_Dash\\.cache\\huggingface\\datasets\\json\\default-5834773ef589f384\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb574528a1714cb4982fe84a28fe8116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'metadata': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"data\\\\rct_sample_train.jsonl\", \"test\": \"data\\\\rct_sample_test.jsonl\"})\n",
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71fbb0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7200392c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Class encoding is only supported for Value column, and column label is ClassLabel.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_encode_column\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:1398\u001B[0m, in \u001B[0;36mDataset.class_encode_column\u001B[1;34m(self, column, include_nulls)\u001B[0m\n\u001B[0;32m   1396\u001B[0m src_feat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[column]\n\u001B[0;32m   1397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(src_feat, Value):\n\u001B[1;32m-> 1398\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1399\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClass encoding is only supported for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mValue\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m column, and column \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(src_feat)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1400\u001B[0m     )\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m src_feat\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (include_nulls \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munique(column)):\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstringify_column\u001B[39m(batch):\n",
      "\u001B[1;31mValueError\u001B[0m: Class encoding is only supported for Value column, and column label is ClassLabel."
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].class_encode_column(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad069d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Class encoding is only supported for Value column, and column label is ClassLabel.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_encode_column\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:1398\u001B[0m, in \u001B[0;36mDataset.class_encode_column\u001B[1;34m(self, column, include_nulls)\u001B[0m\n\u001B[0;32m   1396\u001B[0m src_feat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[column]\n\u001B[0;32m   1397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(src_feat, Value):\n\u001B[1;32m-> 1398\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1399\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClass encoding is only supported for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mValue\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m column, and column \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(src_feat)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1400\u001B[0m     )\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m src_feat\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (include_nulls \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munique(column)):\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstringify_column\u001B[39m(batch):\n",
      "\u001B[1;31mValueError\u001B[0m: Class encoding is only supported for Value column, and column label is ClassLabel."
     ]
    }
   ],
   "source": [
    "dataset['test'] = dataset['test'].class_encode_column(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0824225a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(num_classes=5, names=['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'], id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'metadata': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da8ee55a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Use of the mobile application was greater than in a previous trial and was associated with greater sun protection , especially among women .'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"metadata\"])\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37155281",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Roberta Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ef65d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe86d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fb0a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import ClassLabel\n",
    "# labels = ClassLabel(names=labels_df)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea40e3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    tokenized = tokenizer(batch[\"text\"], padding = \"max_length\", truncation=True)\n",
    "#     tokenized['label'] = labels.str2int(batch['label'])\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cce7da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_citation = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77353d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_citation[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93340feb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=labels_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2d453",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b4ea574",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "To be able to use accuracy, you need to install the following dependency: sklearn.\nPlease install it using 'pip install sklearn' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[1;32m----> 2\u001B[0m metric \u001B[38;5;241m=\u001B[39m \u001B[43mload_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\load.py:1389\u001B[0m, in \u001B[0;36mload_metric\u001B[1;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **metric_init_kwargs)\u001B[0m\n\u001B[0;32m   1353\u001B[0m \u001B[38;5;124;03m\"\"\"Load a `datasets.Metric`.\u001B[39;00m\n\u001B[0;32m   1354\u001B[0m \n\u001B[0;32m   1355\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1386\u001B[0m \u001B[38;5;124;03m```\u001B[39;00m\n\u001B[0;32m   1387\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1388\u001B[0m download_mode \u001B[38;5;241m=\u001B[39m DownloadMode(download_mode \u001B[38;5;129;01mor\u001B[39;00m DownloadMode\u001B[38;5;241m.\u001B[39mREUSE_DATASET_IF_EXISTS)\n\u001B[1;32m-> 1389\u001B[0m metric_module \u001B[38;5;241m=\u001B[39m \u001B[43mmetric_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\n\u001B[0;32m   1391\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmodule_path\n\u001B[0;32m   1392\u001B[0m metric_cls \u001B[38;5;241m=\u001B[39m import_main_class(metric_module, dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1393\u001B[0m metric \u001B[38;5;241m=\u001B[39m metric_cls(\n\u001B[0;32m   1394\u001B[0m     config_name\u001B[38;5;241m=\u001B[39mconfig_name,\n\u001B[0;32m   1395\u001B[0m     process_id\u001B[38;5;241m=\u001B[39mprocess_id,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1400\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmetric_init_kwargs,\n\u001B[0;32m   1401\u001B[0m )\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\load.py:1331\u001B[0m, in \u001B[0;36mmetric_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, **download_kwargs)\u001B[0m\n\u001B[0;32m   1329\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e2:  \u001B[38;5;66;03m# noqa: if it's not in the cache, then it doesn't exist.\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e1, \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m):\n\u001B[1;32m-> 1331\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m e1 \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m   1332\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1333\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a metric script at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(combined_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1334\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMetric \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist on the Hugging Face Hub either.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1335\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m   1336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\load.py:1319\u001B[0m, in \u001B[0;36mmetric_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, **download_kwargs)\u001B[0m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_relative_path(path) \u001B[38;5;129;01mand\u001B[39;00m path\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1318\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1319\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGithubMetricModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1320\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1321\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e1:  \u001B[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\load.py:557\u001B[0m, in \u001B[0;36mGithubMetricModuleFactory.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    552\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    553\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a directory or a metric named \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in this version. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    554\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt was picked from the main branch on github instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    555\u001B[0m         )\n\u001B[0;32m    556\u001B[0m imports \u001B[38;5;241m=\u001B[39m get_imports(local_path)\n\u001B[1;32m--> 557\u001B[0m local_imports \u001B[38;5;241m=\u001B[39m \u001B[43m_download_additional_modules\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    559\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_github_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimports\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimports\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[38;5;66;03m# copy the script and the files in an importable directory\u001B[39;00m\n\u001B[0;32m    564\u001B[0m dynamic_modules_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdynamic_modules_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdynamic_modules_path \u001B[38;5;28;01melse\u001B[39;00m init_dynamic_modules()\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\datasets\\load.py:215\u001B[0m, in \u001B[0;36m_download_additional_modules\u001B[1;34m(name, base_path, imports, download_config)\u001B[0m\n\u001B[0;32m    213\u001B[0m     _depencencies_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdependencies\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(needs_to_be_installed) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdependency\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    214\u001B[0m     _them_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthem\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(needs_to_be_installed) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 215\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m    216\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo be able to use \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, you need to install the following \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_depencencies_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    217\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(needs_to_be_installed)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease install \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_them_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m using \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(needs_to_be_installed\u001B[38;5;241m.\u001B[39mvalues())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for instance\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    219\u001B[0m     )\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m local_imports\n",
      "\u001B[1;31mImportError\u001B[0m: To be able to use accuracy, you need to install the following dependency: sklearn.\nPlease install it using 'pip install sklearn' for instance'"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb73bd90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d00a410",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments(output_dir=\"./results\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=5, weight_decay=0.01,evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7108583",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_citation[\"train\"],\n",
    "    eval_dataset=tokenized_citation[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af6c45",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bbcd3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  Adapter based Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d11b335",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\Sudeshna_Dash/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\Sudeshna_Dash/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Sudeshna_Dash/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98965d3e8bf9445bbe8c79dbc374533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55d4f9742824f958215533de707c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'rename_column_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [35]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mmap(encode_batch, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# The transformers model expects the target class column to be named \"labels\"\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrename_column_\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mrename_column_(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Transform to pytorch tensors and only output the required columns\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Dataset' object has no attribute 'rename_column_'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9007fdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56d605d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Sudeshna_Dash/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Sudeshna_Dash/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaAdapterModel\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=labels_df.size,\n",
    ")\n",
    "model = RobertaAdapterModel.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95443906",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding adapter 'roberta_base_rct'.\n",
      "Adding head 'roberta_base_rct' with config {'head_type': 'classification', 'num_labels': 5, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'CONCLUSIONS': 0, 'RESULTS': 1, 'OBJECTIVE': 2, 'METHODS': 3, 'BACKGROUND': 4}, 'use_pooler': False, 'bias': True}.\n"
     ]
    }
   ],
   "source": [
    "# Add a new adapter\n",
    "model.add_adapter(\"roberta_base_rct\")\n",
    "model.add_classification_head(\n",
    "    \"roberta_base_rct\",\n",
    "    num_labels=labels_df.size,\n",
    "    id2label={ 0: labels_df[0], 1: labels_df[1], 2: labels_df[2], 3: labels_df[3], 4: labels_df[4]}\n",
    "  )\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"roberta_base_rct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb5bd6f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "700cde14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/96 : < :, Epoch 0.06/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [50]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\transformers\\trainer.py:1375\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1373\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[0;32m   1374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1375\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1377\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1378\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   1379\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[0;32m   1380\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   1381\u001B[0m ):\n\u001B[0;32m   1382\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   1383\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\transformers\\trainer.py:1977\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   1975\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed\u001B[38;5;241m.\u001B[39mbackward(loss)\n\u001B[0;32m   1976\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1977\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1979\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61dd2a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ad8cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
    "\n",
    "classifier(\"These results are great for future purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e681920",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"./final_adapter\", \"rotten_tomatoes\")\n",
    "\n",
    "!ls -lh final_adapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}